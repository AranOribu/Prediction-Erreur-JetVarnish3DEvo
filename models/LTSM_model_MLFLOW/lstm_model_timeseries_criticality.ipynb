{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler , OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import pickle\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import ColSpec, TensorSpec\n",
    "from mlflow.types import Schema\n",
    "\n",
    "\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.regularizers import L2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALLAN\\AppData\\Local\\Temp\\ipykernel_49516\\1742999737.py:5: DtypeWarning: Columns (10,11,12,13,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sn_modules</th>\n",
       "      <th>name_modules</th>\n",
       "      <th>type_modules</th>\n",
       "      <th>generation_modules</th>\n",
       "      <th>name_counters_modules</th>\n",
       "      <th>value_counters_modules</th>\n",
       "      <th>name_connected_operators</th>\n",
       "      <th>level_connected_operators</th>\n",
       "      <th>source_events</th>\n",
       "      <th>message_events</th>\n",
       "      <th>timestamp_events</th>\n",
       "      <th>criticality_events</th>\n",
       "      <th>identification_events</th>\n",
       "      <th>created_at</th>\n",
       "      <th>varnishLevelsTargetvolume</th>\n",
       "      <th>varnishLevelsTotalvolume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4169748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>Varnish Printer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-15 05:55:06.678000</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4169748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iFoil L</td>\n",
       "      <td>iFoil</td>\n",
       "      <td>Gen. 2</td>\n",
       "      <td>Total Pages Counter</td>\n",
       "      <td>22881</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-15 05:55:06.678000</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  sn_modules    name_modules     type_modules generation_modules  \\\n",
       "0  4169748         NaN  Print Engine 1  Varnish Printer                NaN   \n",
       "1  4169748         NaN         iFoil L            iFoil             Gen. 2   \n",
       "\n",
       "  name_counters_modules  value_counters_modules name_connected_operators  \\\n",
       "0    3D Varnish Counter                 1792992                   Viktor   \n",
       "1   Total Pages Counter                   22881                   Viktor   \n",
       "\n",
       "  level_connected_operators source_events message_events timestamp_events  \\\n",
       "0                  Operator           NaN            NaN              NaN   \n",
       "1                  Operator           NaN            NaN              NaN   \n",
       "\n",
       "  criticality_events identification_events                  created_at  \\\n",
       "0                NaN                   NaN  2022-04-15 05:55:06.678000   \n",
       "1                NaN                   NaN  2022-04-15 05:55:06.678000   \n",
       "\n",
       "   varnishLevelsTargetvolume  varnishLevelsTotalvolume  \n",
       "0               36192.322612                    100000  \n",
       "1               36192.322612                    100000  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nom de fichier et chemin relatif\n",
    "filename = 'merge_raw_metrics_dataset.csv'\n",
    "path = '../../data/metrics/'\n",
    "# création d'un dataframe à partir du csv de données\n",
    "df = pd.read_csv(\n",
    "    path+filename, index_col=0).sort_values(by='created_at', ascending=True)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sn_modules</th>\n",
       "      <th>name_modules</th>\n",
       "      <th>type_modules</th>\n",
       "      <th>generation_modules</th>\n",
       "      <th>name_counters_modules</th>\n",
       "      <th>value_counters_modules</th>\n",
       "      <th>name_connected_operators</th>\n",
       "      <th>level_connected_operators</th>\n",
       "      <th>source_events</th>\n",
       "      <th>message_events</th>\n",
       "      <th>timestamp_events</th>\n",
       "      <th>criticality_events</th>\n",
       "      <th>identification_events</th>\n",
       "      <th>varnishLevelsTargetvolume</th>\n",
       "      <th>varnishLevelsTotalvolume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.678</th>\n",
       "      <td>4169748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>Varnish Printer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.678</th>\n",
       "      <td>4169748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iFoil L</td>\n",
       "      <td>iFoil</td>\n",
       "      <td>Gen. 2</td>\n",
       "      <td>Total Pages Counter</td>\n",
       "      <td>22881</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.678</th>\n",
       "      <td>4169748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iFoil L</td>\n",
       "      <td>iFoil</td>\n",
       "      <td>Gen. 2</td>\n",
       "      <td>Foiled Pages Counter</td>\n",
       "      <td>31092</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.829</th>\n",
       "      <td>4169749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>Varnish Printer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>PLC</td>\n",
       "      <td>JV-Ti non prêt : impression impossible</td>\n",
       "      <td>2022-04-15T05:55:23.462Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.829</th>\n",
       "      <td>4169749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iFoil L</td>\n",
       "      <td>iFoil</td>\n",
       "      <td>Gen. 2</td>\n",
       "      <td>Total Pages Counter</td>\n",
       "      <td>22881</td>\n",
       "      <td>Viktor</td>\n",
       "      <td>Operator</td>\n",
       "      <td>PLC</td>\n",
       "      <td>JV-Ti non prêt : impression impossible</td>\n",
       "      <td>2022-04-15T05:55:23.462Z</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  sn_modules    name_modules     type_modules  \\\n",
       "created_at                                                                      \n",
       "2022-04-15 05:55:06.678  4169748         NaN  Print Engine 1  Varnish Printer   \n",
       "2022-04-15 05:55:06.678  4169748         NaN         iFoil L            iFoil   \n",
       "2022-04-15 05:55:06.678  4169748         NaN         iFoil L            iFoil   \n",
       "2022-04-15 05:55:06.829  4169749         NaN  Print Engine 1  Varnish Printer   \n",
       "2022-04-15 05:55:06.829  4169749         NaN         iFoil L            iFoil   \n",
       "\n",
       "                        generation_modules name_counters_modules  \\\n",
       "created_at                                                         \n",
       "2022-04-15 05:55:06.678                NaN    3D Varnish Counter   \n",
       "2022-04-15 05:55:06.678             Gen. 2   Total Pages Counter   \n",
       "2022-04-15 05:55:06.678             Gen. 2  Foiled Pages Counter   \n",
       "2022-04-15 05:55:06.829                NaN    3D Varnish Counter   \n",
       "2022-04-15 05:55:06.829             Gen. 2   Total Pages Counter   \n",
       "\n",
       "                         value_counters_modules name_connected_operators  \\\n",
       "created_at                                                                 \n",
       "2022-04-15 05:55:06.678                 1792992                   Viktor   \n",
       "2022-04-15 05:55:06.678                   22881                   Viktor   \n",
       "2022-04-15 05:55:06.678                   31092                   Viktor   \n",
       "2022-04-15 05:55:06.829                 1792992                   Viktor   \n",
       "2022-04-15 05:55:06.829                   22881                   Viktor   \n",
       "\n",
       "                        level_connected_operators source_events  \\\n",
       "created_at                                                        \n",
       "2022-04-15 05:55:06.678                  Operator           NaN   \n",
       "2022-04-15 05:55:06.678                  Operator           NaN   \n",
       "2022-04-15 05:55:06.678                  Operator           NaN   \n",
       "2022-04-15 05:55:06.829                  Operator           PLC   \n",
       "2022-04-15 05:55:06.829                  Operator           PLC   \n",
       "\n",
       "                                                  message_events  \\\n",
       "created_at                                                         \n",
       "2022-04-15 05:55:06.678                                      NaN   \n",
       "2022-04-15 05:55:06.678                                      NaN   \n",
       "2022-04-15 05:55:06.678                                      NaN   \n",
       "2022-04-15 05:55:06.829   JV-Ti non prêt : impression impossible   \n",
       "2022-04-15 05:55:06.829   JV-Ti non prêt : impression impossible   \n",
       "\n",
       "                                 timestamp_events criticality_events  \\\n",
       "created_at                                                             \n",
       "2022-04-15 05:55:06.678                       NaN                NaN   \n",
       "2022-04-15 05:55:06.678                       NaN                NaN   \n",
       "2022-04-15 05:55:06.678                       NaN                NaN   \n",
       "2022-04-15 05:55:06.829  2022-04-15T05:55:23.462Z               INFO   \n",
       "2022-04-15 05:55:06.829  2022-04-15T05:55:23.462Z               INFO   \n",
       "\n",
       "                        identification_events  varnishLevelsTargetvolume  \\\n",
       "created_at                                                                 \n",
       "2022-04-15 05:55:06.678                   NaN               36192.322612   \n",
       "2022-04-15 05:55:06.678                   NaN               36192.322612   \n",
       "2022-04-15 05:55:06.678                   NaN               36192.322612   \n",
       "2022-04-15 05:55:06.829                   391               36192.322612   \n",
       "2022-04-15 05:55:06.829                   391               36192.322612   \n",
       "\n",
       "                         varnishLevelsTotalvolume  \n",
       "created_at                                         \n",
       "2022-04-15 05:55:06.678                    100000  \n",
       "2022-04-15 05:55:06.678                    100000  \n",
       "2022-04-15 05:55:06.678                    100000  \n",
       "2022-04-15 05:55:06.829                    100000  \n",
       "2022-04-15 05:55:06.829                    100000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "df = df.sort_values(by='created_at')\n",
    "df.set_index('created_at', inplace=True)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sn_modules', 'name_modules', 'type_modules',\n",
       "       'generation_modules', 'name_counters_modules', 'value_counters_modules',\n",
       "       'name_connected_operators', 'level_connected_operators',\n",
       "       'source_events', 'message_events', 'timestamp_events',\n",
       "       'criticality_events', 'identification_events',\n",
       "       'varnishLevelsTargetvolume', 'varnishLevelsTotalvolume'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3546276, 16)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values:\n",
      "sn_modules               True\n",
      "generation_modules       True\n",
      "source_events            True\n",
      "message_events           True\n",
      "timestamp_events         True\n",
      "criticality_events       True\n",
      "identification_events    True\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "null_columns = df.isna().any()\n",
    "print(\"Columns with missing values:\")\n",
    "print(null_columns[null_columns == True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3546276, 16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cela va supprimer la colonne sn car vide\n",
    "df.dropna(axis='columns', how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['varnishLevelsTotalvolume'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['generation_modules'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime les colonnes :\n",
    "# message events car redondante avec la colonne identification_events \n",
    "# id , name_connected_operators, level_connected_operators car inutiles pour entrainer le model\n",
    "# timestamp_events : on a déjà une date pour l'index qui servi pour les prediction dans le temps\n",
    "# A VERIFIER : generation_modules , la colonne dispose d'une seul valeur qui est 'gen2' \n",
    "# 'name_modules' information redondante avec 'type_modules'\n",
    "df.drop(columns=['message_events','type_modules','id','name_connected_operators','level_connected_operators','generation_modules','timestamp_events'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3546276, 8)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '391', '330', '332', '377', '333', '334', '331',\n",
       "       'Kernel_Error', '315', '417', '406', '407', '352', '344',\n",
       "       'ICB communication error', '376', '445', '325', '343', '345',\n",
       "       '358', '453', '381', '354', '313', '447', '454', '387', '386',\n",
       "       '372', '371', '323', '480', '311', '479', '351', '440', '324',\n",
       "       '321', '0', '349', 'RCB communication error', '385', '357', '418',\n",
       "       '446', '355', '389', '476', '356', 'iFoil communication error',\n",
       "       '460', '472', '405', '380', '388', '408', 445.0, 391.0, 330.0,\n",
       "       333.0, 408.0, 407.0, 406.0, 332.0, 334.0, 472.0, 331.0, 352.0,\n",
       "       '320', '329', '350', '475', '466', '416', '411', '346', '471',\n",
       "       '327', 430.0, '430', '444', '2', '326', '419',\n",
       "       'Pilot communication error', '359', 313.0, 377.0, 453.0, 376.0,\n",
       "       344.0, 325.0, 454.0, 315.0, 417.0, '322', 385.0, 371.0, 386.0,\n",
       "       '384'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['identification_events'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the mapping for non-integer strings\n",
    "non_int_string_mapping = {}\n",
    "next_mapping_value = 1000\n",
    "# 'Kernel_Error' = 1000 , 'ICB communication error' = 1001 ; 'RCB communication error' = 1002 , 'iFoil communication error' = 1003 , 'Pilot communication error' = \n",
    "\n",
    "# Function to convert the value\n",
    "def convert_value(value):\n",
    "    global next_mapping_value\n",
    "\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "\n",
    "    if isinstance(value, (int, float)):\n",
    "        return int(value)\n",
    "    \n",
    "    if value.isdigit():\n",
    "        return int(value)\n",
    "    \n",
    "    if value not in non_int_string_mapping:\n",
    "        non_int_string_mapping[value] = next_mapping_value\n",
    "        next_mapping_value += 1\n",
    "        \n",
    "    return non_int_string_mapping[value]\n",
    "\n",
    "# Apply the conversion function to the 'identification_events' column\n",
    "df['identification_events'] = df['identification_events'].apply(convert_value)\n",
    "\n",
    "# Convert the column to integers, keeping NaN values as float\n",
    "df['identification_events'] = df['identification_events'].astype(pd.Int64Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IntegerArray>\n",
       "[<NA>,  391,  330,  332,  377,  333,  334,  331, 1000,  315,  417,  406,  407,\n",
       "  352,  344, 1001,  376,  445,  325,  343,  345,  358,  453,  381,  354,  313,\n",
       "  447,  454,  387,  386,  372,  371,  323,  480,  311,  479,  351,  440,  324,\n",
       "  321,    0,  349, 1002,  385,  357,  418,  446,  355,  389,  476,  356, 1003,\n",
       "  460,  472,  405,  380,  388,  408,  320,  329,  350,  475,  466,  416,  411,\n",
       "  346,  471,  327,  430,  444,    2,  326,  419, 1004,  359,  322,  384]\n",
       "Length: 77, dtype: Int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['identification_events'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'PLC', 'iFoil', 'Kernel', 'ICB n°5', 'RCB n°1', 'RCB n°2',\n",
       "       'RCB n°3', 'ICB n°7', 'ICB n°4', 'ICB n°8', 'ICB n°2', 'ICB n°1',\n",
       "       'ICB n°6', 'Pilot'], dtype=object)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source_events'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_events: 3302193 valeurs manquantes sur 3546276\n",
      "criticality_events: 3302193 valeurs manquantes sur 3546276\n",
      "identification_events: 3302193 valeurs manquantes sur 3546276\n"
     ]
    }
   ],
   "source": [
    "# count null values in each column\n",
    "null_values_count = df.isnull().sum()\n",
    "\n",
    "for column, value in null_values_count.items():\n",
    "    if value > 0:\n",
    "        print(f\"{column}: {value} valeurs manquantes sur {df.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INFO       179610\n",
       "WARNING     34497\n",
       "ERROR       29976\n",
       "Name: criticality_events, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['criticality_events'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name_modules', 'name_counters_modules', 'source_events']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify categorical columns (excluding the target column 'criticality')\n",
    "categorical_columns = ['name_modules',  'name_counters_modules', 'source_events']\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with 'nan' in 'criticality_events'\n",
    "df.dropna(subset=['criticality_events'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-04-15 05:55:06.829000', '2022-04-15 05:55:06.829000',\n",
       "               '2022-04-15 05:55:06.829000', '2022-04-15 06:06:35.404000',\n",
       "               '2022-04-15 06:06:35.404000', '2022-04-15 06:06:35.404000',\n",
       "               '2022-04-15 06:06:35.404000', '2022-04-15 06:06:35.404000',\n",
       "               '2022-04-15 06:06:35.404000', '2022-04-15 06:07:05.443000',\n",
       "               ...\n",
       "               '2022-12-12 08:19:07.632000', '2022-12-12 08:19:48.688000',\n",
       "               '2022-12-12 08:19:48.688000', '2022-12-12 08:19:48.688000',\n",
       "               '2022-12-12 08:20:17.777000', '2022-12-12 08:20:17.777000',\n",
       "               '2022-12-12 08:20:17.777000', '2022-12-12 08:21:18.076000',\n",
       "               '2022-12-12 08:21:18.076000', '2022-12-12 08:21:18.076000'],\n",
       "              dtype='datetime64[ns]', name='created_at', length=244083, freq=None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select the last row from `X_test` (before encoding) for an MLFLOW input example\n",
    "last_row_enxample = df.iloc[-1]\n",
    "\n",
    "# 2. Create a JSON object using the original row and column names\n",
    "example_input_json = last_row_enxample.to_dict()\n",
    "\n",
    "# Save the JSON object to a file\n",
    "import json\n",
    "with open(\"example_input.json\", \"w\") as f:\n",
    "    json.dump(example_input_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_modules</th>\n",
       "      <th>name_counters_modules</th>\n",
       "      <th>value_counters_modules</th>\n",
       "      <th>source_events</th>\n",
       "      <th>criticality_events</th>\n",
       "      <th>identification_events</th>\n",
       "      <th>varnishLevelsTargetvolume</th>\n",
       "      <th>varnishLevelsTotalvolume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>created_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.829</th>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>PLC</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.829</th>\n",
       "      <td>iFoil L</td>\n",
       "      <td>Total Pages Counter</td>\n",
       "      <td>22881</td>\n",
       "      <td>PLC</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 05:55:06.829</th>\n",
       "      <td>iFoil L</td>\n",
       "      <td>Foiled Pages Counter</td>\n",
       "      <td>31092</td>\n",
       "      <td>PLC</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 06:06:35.404</th>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>iFoil</td>\n",
       "      <td>INFO</td>\n",
       "      <td>391</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-15 06:06:35.404</th>\n",
       "      <td>Print Engine 1</td>\n",
       "      <td>3D Varnish Counter</td>\n",
       "      <td>1792992</td>\n",
       "      <td>PLC</td>\n",
       "      <td>INFO</td>\n",
       "      <td>330</td>\n",
       "      <td>36192.322612</td>\n",
       "      <td>100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name_modules name_counters_modules  \\\n",
       "created_at                                                      \n",
       "2022-04-15 05:55:06.829  Print Engine 1    3D Varnish Counter   \n",
       "2022-04-15 05:55:06.829         iFoil L   Total Pages Counter   \n",
       "2022-04-15 05:55:06.829         iFoil L  Foiled Pages Counter   \n",
       "2022-04-15 06:06:35.404  Print Engine 1    3D Varnish Counter   \n",
       "2022-04-15 06:06:35.404  Print Engine 1    3D Varnish Counter   \n",
       "\n",
       "                         value_counters_modules source_events  \\\n",
       "created_at                                                      \n",
       "2022-04-15 05:55:06.829                 1792992           PLC   \n",
       "2022-04-15 05:55:06.829                   22881           PLC   \n",
       "2022-04-15 05:55:06.829                   31092           PLC   \n",
       "2022-04-15 06:06:35.404                 1792992         iFoil   \n",
       "2022-04-15 06:06:35.404                 1792992           PLC   \n",
       "\n",
       "                        criticality_events  identification_events  \\\n",
       "created_at                                                          \n",
       "2022-04-15 05:55:06.829               INFO                    391   \n",
       "2022-04-15 05:55:06.829               INFO                    391   \n",
       "2022-04-15 05:55:06.829               INFO                    391   \n",
       "2022-04-15 06:06:35.404               INFO                    391   \n",
       "2022-04-15 06:06:35.404               INFO                    330   \n",
       "\n",
       "                         varnishLevelsTargetvolume  varnishLevelsTotalvolume  \n",
       "created_at                                                                    \n",
       "2022-04-15 05:55:06.829               36192.322612                    100000  \n",
       "2022-04-15 05:55:06.829               36192.322612                    100000  \n",
       "2022-04-15 05:55:06.829               36192.322612                    100000  \n",
       "2022-04-15 06:06:35.404               36192.322612                    100000  \n",
       "2022-04-15 06:06:35.404               36192.322612                    100000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "\n",
    "# profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\HubAcademy\\AI\\MGI\\Prediction-Erreur-JetVarnish3D\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical columns except for 'criticality_events'\n",
    "cat_columns = ['name_modules', 'name_counters_modules', 'source_events']\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the categorical columns\n",
    "cat_data_encoded = ohe.fit_transform(df[cat_columns])\n",
    "\n",
    "# Convert the encoded data to a DataFrame\n",
    "cat_data_encoded_df = pd.DataFrame(cat_data_encoded, columns=ohe.get_feature_names_out(cat_columns))\n",
    "\n",
    "# Drop the original categorical columns from the DataFrame\n",
    "df_encoded = df.drop(cat_columns, axis=1)\n",
    "\n",
    "# Reset the index of both DataFrames to avoid index-related issues\n",
    "df_encoded.reset_index(drop=True, inplace=True)\n",
    "cat_data_encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the one-hot encoded DataFrame with the original DataFrame\n",
    "df_encoded = pd.concat([df_encoded, cat_data_encoded_df], axis=1)\n",
    "\n",
    "# Label encode 'criticality_events' column\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(['INFO', 'WARNING', 'ERROR'])\n",
    "df_encoded['criticality_events'] = le.fit_transform(df_encoded['criticality_events'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical columns\n",
    "numerical_columns = ['value_counters_modules', 'identification_events', 'varnishLevelsTargetvolume', 'varnishLevelsTotalvolume']\n",
    "scaler = MinMaxScaler()\n",
    "df_encoded[numerical_columns] = scaler.fit_transform(df_encoded[numerical_columns])\n",
    "\n",
    "# Split dataset into X and y\n",
    "X = df_encoded.drop('criticality_events', axis=1)\n",
    "y = df_encoded['criticality_events']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['value_counters_modules', 'identification_events',\n",
       "       'varnishLevelsTargetvolume', 'varnishLevelsTotalvolume',\n",
       "       'name_modules_Print Engine 1', 'name_modules_iFoil L',\n",
       "       'name_counters_modules_3D Varnish Counter',\n",
       "       'name_counters_modules_Foiled Pages Counter',\n",
       "       'name_counters_modules_Total Pages Counter', 'source_events_ICB n°1',\n",
       "       'source_events_ICB n°2', 'source_events_ICB n°4',\n",
       "       'source_events_ICB n°5', 'source_events_ICB n°6',\n",
       "       'source_events_ICB n°7', 'source_events_ICB n°8',\n",
       "       'source_events_Kernel', 'source_events_PLC', 'source_events_Pilot',\n",
       "       'source_events_RCB n°1', 'source_events_RCB n°2',\n",
       "       'source_events_RCB n°3', 'source_events_iFoil'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244083, 23)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences\n",
    "def create_sequences(X_data, y_data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(X_data) - seq_length):\n",
    "        x = X_data.iloc[i:(i + seq_length)].values\n",
    "        y = y_data.iloc[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Store the original index information for X_test and y_test\n",
    "X_test_index = X_test.index\n",
    "y_test_index = y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data into sequences of the specified length\n",
    "sequence_length = 1\n",
    "X_train, y_train = create_sequences(X_train, y_train, sequence_length)\n",
    "X_test, y_test = create_sequences(X_test, y_test, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input schema\n",
    "input_schema = Schema([\n",
    "    TensorSpec(type=np.dtype(np.float64), shape=(-1, X_train.shape[1], X_train.shape[2]), name='input')\n",
    "])\n",
    "\n",
    "# Define the output schema\n",
    "output_schema = Schema([\n",
    "    ColSpec(type='integer', name='output')\n",
    "])\n",
    "\n",
    "# Create the signature\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40679, 1, 23)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203402, 1, 23)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régularisation L2 est une technique utilisée pour éviter le surajustement en ajoutant un terme de pénalité à la fonction de perte. Ce terme de pénalité est proportionnel au carré des poids du modèle. En ajoutant la régularisation L2, le modèle est encouragé à apprendre des poids plus petits et plus épars, ce qui permet d'obtenir un modèle plus simple et plus généralisé.\n",
    "\n",
    "Dans l'exemple fourni, j'ai ajouté une régularisation L2 aux couches LSTM à l'aide des paramètres kernel_regularizer et recurrent_regularizer. Voici une brève explication de ces paramètres :\n",
    "\n",
    "- kernel_regularizer : Ce paramètre applique la régularisation L2 aux poids de l'entrée vers la couche cachée (également connus sous le nom de poids \"noyau\") de la couche LSTM. En d'autres termes, il ajoute un terme de pénalité basé sur le carré de ces poids à la fonction de perte.\n",
    "- recurrent_regularizer : Cette fonction applique une régularisation L2 aux poids cachés-cachés (récurrents) de la couche LSTM. De même, il ajoute un terme de pénalité basé sur le carré de ces poids à la fonction de perte.\n",
    "En ajoutant la régularisation L2 au noyau et aux poids récurrents, le modèle est encouragé à apprendre des poids plus petits, ce qui permet de réduire l'ajustement excessif et d'améliorer la généralisation.\n",
    "\n",
    "N'oubliez pas que la force de la régularisation est contrôlée par l'hyperparamètre lambda (noté 0,001 dans l'exemple). Une valeur plus élevée de lambda se traduit par un effet de régularisation plus important, mais une valeur trop élevée peut entraîner un sous-ajustement, car le modèle devient trop simple pour apprendre des modèles complexes dans les données. Il se peut que vous deviez expérimenter différentes valeurs de lambda pour trouver la force de régularisation optimale pour votre problème spécifique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With validation_split=0.1, the last 10% of the training data is used as the validation set, and the remaining 90% is used for training the model. Since we've set shuffle=False, the validation data consists of the most recent 10% of the training data, preserving the chronological order.\n",
    "\n",
    "This way, both the validation and test sets consist of later data points in time, which is a more realistic approach for time series prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build an LSTM model with added regularization\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True, kernel_regularizer=L2(0.0001), recurrent_regularizer=L2(0.001), dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(LSTM(64, return_sequences=True, kernel_regularizer=L2(0.0001), recurrent_regularizer=L2(0.0001), dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(LSTM(32, return_sequences=True, kernel_regularizer=L2(0.0001), recurrent_regularizer=L2(0.0001), dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(LSTM(16, kernel_regularizer=L2(0.0001), recurrent_regularizer=L2(0.0001), dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.3))\n",
    "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Define model hyperparameters\n",
    "patience = 10\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Set the optimizer with the specified learning_rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "checkpoint = ModelCheckpoint('model_checkpoint.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.7685 - accuracy: 0.7366\n",
      "Epoch 1: val_loss improved from inf to 0.71586, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 25s 13ms/step - loss: 0.7683 - accuracy: 0.7367 - val_loss: 0.7159 - val_accuracy: 0.7560\n",
      "Epoch 2/50\n",
      "1430/1431 [============================>.] - ETA: 0s - loss: 0.7458 - accuracy: 0.7435\n",
      "Epoch 2: val_loss improved from 0.71586 to 0.70976, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 18s 12ms/step - loss: 0.7458 - accuracy: 0.7435 - val_loss: 0.7098 - val_accuracy: 0.7580\n",
      "Epoch 3/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.7270 - accuracy: 0.7448\n",
      "Epoch 3: val_loss did not improve from 0.70976\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.7266 - accuracy: 0.7450 - val_loss: 0.7478 - val_accuracy: 0.7604\n",
      "Epoch 4/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.7521\n",
      "Epoch 4: val_loss improved from 0.70976 to 0.70702, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6900 - accuracy: 0.7522 - val_loss: 0.7070 - val_accuracy: 0.7658\n",
      "Epoch 5/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6762 - accuracy: 0.7549\n",
      "Epoch 5: val_loss improved from 0.70702 to 0.70196, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6762 - accuracy: 0.7549 - val_loss: 0.7020 - val_accuracy: 0.7649\n",
      "Epoch 6/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.7538\n",
      "Epoch 6: val_loss improved from 0.70196 to 0.69868, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6843 - accuracy: 0.7538 - val_loss: 0.6987 - val_accuracy: 0.7652\n",
      "Epoch 7/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.6555 - accuracy: 0.7592\n",
      "Epoch 7: val_loss improved from 0.69868 to 0.62975, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6552 - accuracy: 0.7593 - val_loss: 0.6298 - val_accuracy: 0.7782\n",
      "Epoch 8/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6366 - accuracy: 0.7679\n",
      "Epoch 8: val_loss improved from 0.62975 to 0.60789, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6365 - accuracy: 0.7680 - val_loss: 0.6079 - val_accuracy: 0.7887\n",
      "Epoch 9/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.6280 - accuracy: 0.7737\n",
      "Epoch 9: val_loss did not improve from 0.60789\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6276 - accuracy: 0.7739 - val_loss: 0.6417 - val_accuracy: 0.7684\n",
      "Epoch 10/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6311 - accuracy: 0.7736\n",
      "Epoch 10: val_loss did not improve from 0.60789\n",
      "1431/1431 [==============================] - 18s 12ms/step - loss: 0.6310 - accuracy: 0.7737 - val_loss: 0.6134 - val_accuracy: 0.7853\n",
      "Epoch 11/50\n",
      "1430/1431 [============================>.] - ETA: 0s - loss: 0.6206 - accuracy: 0.7775\n",
      "Epoch 11: val_loss did not improve from 0.60789\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6205 - accuracy: 0.7775 - val_loss: 0.6168 - val_accuracy: 0.7852\n",
      "Epoch 12/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.7793\n",
      "Epoch 12: val_loss improved from 0.60789 to 0.58549, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6150 - accuracy: 0.7793 - val_loss: 0.5855 - val_accuracy: 0.7934\n",
      "Epoch 13/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.6158 - accuracy: 0.7787\n",
      "Epoch 13: val_loss did not improve from 0.58549\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6154 - accuracy: 0.7789 - val_loss: 0.5867 - val_accuracy: 0.7931\n",
      "Epoch 14/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6110 - accuracy: 0.7813\n",
      "Epoch 14: val_loss improved from 0.58549 to 0.57888, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 18s 12ms/step - loss: 0.6109 - accuracy: 0.7814 - val_loss: 0.5789 - val_accuracy: 0.7925\n",
      "Epoch 15/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.6531 - accuracy: 0.7643\n",
      "Epoch 15: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6529 - accuracy: 0.7645 - val_loss: 0.6538 - val_accuracy: 0.7733\n",
      "Epoch 16/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.6745 - accuracy: 0.7574\n",
      "Epoch 16: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6741 - accuracy: 0.7576 - val_loss: 0.6361 - val_accuracy: 0.7773\n",
      "Epoch 17/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6546 - accuracy: 0.7571\n",
      "Epoch 17: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6546 - accuracy: 0.7571 - val_loss: 0.6020 - val_accuracy: 0.7778\n",
      "Epoch 18/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.6377 - accuracy: 0.7613\n",
      "Epoch 18: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6373 - accuracy: 0.7615 - val_loss: 0.5957 - val_accuracy: 0.7862\n",
      "Epoch 19/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6247 - accuracy: 0.7707\n",
      "Epoch 19: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6247 - accuracy: 0.7707 - val_loss: 0.5930 - val_accuracy: 0.7911\n",
      "Epoch 20/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6169 - accuracy: 0.7744\n",
      "Epoch 20: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6169 - accuracy: 0.7744 - val_loss: 0.6217 - val_accuracy: 0.7890\n",
      "Epoch 21/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6307 - accuracy: 0.7711\n",
      "Epoch 21: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6306 - accuracy: 0.7712 - val_loss: 0.6185 - val_accuracy: 0.7849\n",
      "Epoch 22/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.6290 - accuracy: 0.7710\n",
      "Epoch 22: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6285 - accuracy: 0.7712 - val_loss: 0.5810 - val_accuracy: 0.7925\n",
      "Epoch 23/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6118 - accuracy: 0.7769\n",
      "Epoch 23: val_loss did not improve from 0.57888\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6117 - accuracy: 0.7770 - val_loss: 0.5809 - val_accuracy: 0.7928\n",
      "Epoch 24/50\n",
      "1426/1431 [============================>.] - ETA: 0s - loss: 0.6098 - accuracy: 0.7770\n",
      "Epoch 24: val_loss improved from 0.57888 to 0.57695, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6094 - accuracy: 0.7773 - val_loss: 0.5770 - val_accuracy: 0.7922\n",
      "Epoch 25/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6043 - accuracy: 0.7791\n",
      "Epoch 25: val_loss improved from 0.57695 to 0.57496, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6043 - accuracy: 0.7791 - val_loss: 0.5750 - val_accuracy: 0.7922\n",
      "Epoch 26/50\n",
      "1426/1431 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.7788\n",
      "Epoch 26: val_loss improved from 0.57496 to 0.57242, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6055 - accuracy: 0.7791 - val_loss: 0.5724 - val_accuracy: 0.7930\n",
      "Epoch 27/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.6082 - accuracy: 0.7777\n",
      "Epoch 27: val_loss did not improve from 0.57242\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6081 - accuracy: 0.7777 - val_loss: 0.6885 - val_accuracy: 0.7723\n",
      "Epoch 28/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.6035 - accuracy: 0.7800\n",
      "Epoch 28: val_loss did not improve from 0.57242\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6035 - accuracy: 0.7800 - val_loss: 0.5771 - val_accuracy: 0.7930\n",
      "Epoch 29/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.6011 - accuracy: 0.7809\n",
      "Epoch 29: val_loss did not improve from 0.57242\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6009 - accuracy: 0.7810 - val_loss: 0.5901 - val_accuracy: 0.7921\n",
      "Epoch 30/50\n",
      "1430/1431 [============================>.] - ETA: 0s - loss: 0.6003 - accuracy: 0.7809\n",
      "Epoch 30: val_loss did not improve from 0.57242\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.6003 - accuracy: 0.7809 - val_loss: 0.6044 - val_accuracy: 0.7869\n",
      "Epoch 31/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.5979 - accuracy: 0.7824\n",
      "Epoch 31: val_loss did not improve from 0.57242\n",
      "1431/1431 [==============================] - 17s 12ms/step - loss: 0.5977 - accuracy: 0.7825 - val_loss: 0.5746 - val_accuracy: 0.7936\n",
      "Epoch 32/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.5984 - accuracy: 0.7818\n",
      "Epoch 32: val_loss improved from 0.57242 to 0.57040, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5983 - accuracy: 0.7819 - val_loss: 0.5704 - val_accuracy: 0.7948\n",
      "Epoch 33/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5993 - accuracy: 0.7818\n",
      "Epoch 33: val_loss did not improve from 0.57040\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5993 - accuracy: 0.7818 - val_loss: 0.5730 - val_accuracy: 0.7945\n",
      "Epoch 34/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7825\n",
      "Epoch 34: val_loss improved from 0.57040 to 0.57039, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5966 - accuracy: 0.7827 - val_loss: 0.5704 - val_accuracy: 0.7947\n",
      "Epoch 35/50\n",
      "1430/1431 [============================>.] - ETA: 0s - loss: 0.5971 - accuracy: 0.7832\n",
      "Epoch 35: val_loss did not improve from 0.57039\n",
      "1431/1431 [==============================] - 18s 13ms/step - loss: 0.5971 - accuracy: 0.7832 - val_loss: 0.5729 - val_accuracy: 0.7955\n",
      "Epoch 36/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5979 - accuracy: 0.7834\n",
      "Epoch 36: val_loss improved from 0.57039 to 0.56911, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5979 - accuracy: 0.7834 - val_loss: 0.5691 - val_accuracy: 0.7948\n",
      "Epoch 37/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5945 - accuracy: 0.7846\n",
      "Epoch 37: val_loss improved from 0.56911 to 0.56822, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5945 - accuracy: 0.7846 - val_loss: 0.5682 - val_accuracy: 0.7954\n",
      "Epoch 38/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.5970 - accuracy: 0.7837\n",
      "Epoch 38: val_loss did not improve from 0.56822\n",
      "1431/1431 [==============================] - 18s 13ms/step - loss: 0.5966 - accuracy: 0.7839 - val_loss: 0.5823 - val_accuracy: 0.7927\n",
      "Epoch 39/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.6007 - accuracy: 0.7825\n",
      "Epoch 39: val_loss did not improve from 0.56822\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.6005 - accuracy: 0.7826 - val_loss: 0.5733 - val_accuracy: 0.7941\n",
      "Epoch 40/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.5966 - accuracy: 0.7839\n",
      "Epoch 40: val_loss did not improve from 0.56822\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5965 - accuracy: 0.7839 - val_loss: 0.5770 - val_accuracy: 0.7938\n",
      "Epoch 41/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.7843\n",
      "Epoch 41: val_loss did not improve from 0.56822\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5941 - accuracy: 0.7843 - val_loss: 0.6063 - val_accuracy: 0.7876\n",
      "Epoch 42/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.5981 - accuracy: 0.7828\n",
      "Epoch 42: val_loss improved from 0.56822 to 0.56712, saving model to model_checkpoint.h5\n",
      "1431/1431 [==============================] - 18s 13ms/step - loss: 0.5979 - accuracy: 0.7829 - val_loss: 0.5671 - val_accuracy: 0.7982\n",
      "Epoch 43/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5944 - accuracy: 0.7851\n",
      "Epoch 43: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5944 - accuracy: 0.7851 - val_loss: 0.5739 - val_accuracy: 0.7998\n",
      "Epoch 44/50\n",
      "1430/1431 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7851\n",
      "Epoch 44: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5932 - accuracy: 0.7851 - val_loss: 0.5703 - val_accuracy: 0.8017\n",
      "Epoch 45/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7860\n",
      "Epoch 45: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5927 - accuracy: 0.7860 - val_loss: 0.5709 - val_accuracy: 0.8009\n",
      "Epoch 46/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.5945 - accuracy: 0.7847\n",
      "Epoch 46: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5944 - accuracy: 0.7848 - val_loss: 0.5798 - val_accuracy: 0.7925\n",
      "Epoch 47/50\n",
      "1428/1431 [============================>.] - ETA: 0s - loss: 0.5942 - accuracy: 0.7856\n",
      "Epoch 47: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 19s 13ms/step - loss: 0.5941 - accuracy: 0.7857 - val_loss: 0.5855 - val_accuracy: 0.7950\n",
      "Epoch 48/50\n",
      "1431/1431 [==============================] - ETA: 0s - loss: 0.5929 - accuracy: 0.7861\n",
      "Epoch 48: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 21s 15ms/step - loss: 0.5929 - accuracy: 0.7861 - val_loss: 0.5745 - val_accuracy: 0.8003\n",
      "Epoch 49/50\n",
      "1429/1431 [============================>.] - ETA: 0s - loss: 0.5927 - accuracy: 0.7870\n",
      "Epoch 49: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 24s 17ms/step - loss: 0.5926 - accuracy: 0.7871 - val_loss: 0.5750 - val_accuracy: 0.8022\n",
      "Epoch 50/50\n",
      "1427/1431 [============================>.] - ETA: 0s - loss: 0.5954 - accuracy: 0.7854\n",
      "Epoch 50: val_loss did not improve from 0.56712\n",
      "1431/1431 [==============================] - 55s 38ms/step - loss: 0.5950 - accuracy: 0.7855 - val_loss: 0.5684 - val_accuracy: 0.8015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ALLAN\\AppData\\Local\\Temp\\tmpqw9j1o3f\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ALLAN\\AppData\\Local\\Temp\\tmpqw9j1o3f\\model\\data\\model\\assets\n",
      "p:\\HubAcademy\\AI\\MGI\\Prediction-Erreur-JetVarnish3D\\venv\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Start MLflow tracking\n",
    "mlflow.start_run()\n",
    "\n",
    "# Log the model's parameters\n",
    "mlflow.log_param('sequence_length', sequence_length)\n",
    "mlflow.log_param('batch_size', batch_size)\n",
    "mlflow.log_param('epochs', epochs)\n",
    "mlflow.log_param('learning_rate', learning_rate)\n",
    "mlflow.log_param('early_stopping_patience', patience)\n",
    "\n",
    "# Train the LSTM model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_split=0.1, \n",
    "                    callbacks=[early_stop, checkpoint], \n",
    "                    shuffle=False)\n",
    "\n",
    "# Log the final training loss and accuracy\n",
    "train_loss = history.history['loss'][-1]\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "mlflow.log_metric('train_loss', train_loss)\n",
    "mlflow.log_metric('train_accuracy', train_accuracy)\n",
    "\n",
    "# Log the final validation loss and accuracy\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "mlflow.log_metric('val_loss', val_loss)\n",
    "mlflow.log_metric('val_accuracy', val_accuracy)\n",
    "\n",
    "# Save the trained model with the schema\n",
    "mlflow.keras.log_model(model, 'trained_model', signature=signature)\n",
    "\n",
    "\n",
    "# Save the encoders and normalizer\n",
    "joblib.dump(ohe, 'ohe.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "mlflow.log_artifact('ohe.pkl')\n",
    "mlflow.log_artifact('scaler.pkl')\n",
    "\n",
    "# Log the example input JSON file as an artifact\n",
    "mlflow.log_artifact(\"example_input.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('history2.pkl', 'wb') as f:\n",
    "#     pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the best mode\n",
    "# model = load_model('LTSM_model_1_93ACC/lstm_model.h5')\n",
    "\n",
    "# # Load the saved training history:\n",
    "# with open('LTSM_model_1_93ACC/history.pkl', 'rb') as f:\n",
    "#     history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272/1272 [==============================] - 5s 4ms/step\n",
      "Accuracy: 0.7979055532338553\n",
      "Loss: 0.5716685056686401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ERROR       0.62      0.40      0.48      4494\n",
      "        INFO       0.83      0.95      0.88     30233\n",
      "     WARNING       0.64      0.35      0.45      5952\n",
      "\n",
      "    accuracy                           0.80     40679\n",
      "   macro avg       0.69      0.56      0.61     40679\n",
      "weighted avg       0.78      0.80      0.78     40679\n",
      "\n",
      "[[ 1792  2499   203]\n",
      " [  650 28609   974]\n",
      " [  462  3433  2057]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "# Inverse transform the predictions and true labels\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "# Calculate and print accuracy, classification report, and confusion matrix\n",
    "acc = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "# Calculate loss score on test set\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test loss score\n",
    "print(f'Loss: {loss[0]}')\n",
    "\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "print(confusion_matrix(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Calculate the classification report\n",
    "report = classification_report(y_test_labels, y_pred_labels, output_dict=True)\n",
    "report_mlflow = classification_report(y_test_labels, y_pred_labels)\n",
    "\n",
    "with open(\"classification_report.txt\", \"w\") as f:\n",
    "    f.write(report_mlflow)\n",
    "\n",
    "# Extract the metrics for the 'error' class\n",
    "error_precision = report['ERROR']['precision']\n",
    "error_recall = report['ERROR']['recall']\n",
    "error_f1_score = report['ERROR']['f1-score']\n",
    "\n",
    "\n",
    "# Log the metrics to MLflow\n",
    "mlflow.log_metric('error_precision', error_precision)\n",
    "mlflow.log_metric('error_recall', error_recall)\n",
    "mlflow.log_metric('error_f1_score', error_f1_score)\n",
    "\n",
    "# Log the classification report \n",
    "mlflow.log_artifact('classification_report.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272/1272 [==============================] - 5s 4ms/step\n",
      "Accuracy: 0.7979055532338553\n",
      "Loss: 0.5716685056686401\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "# Inverse transform the predictions and true labels\n",
    "y_pred_labels = le.inverse_transform(y_pred)\n",
    "y_test_labels = le.inverse_transform(y_test)\n",
    "\n",
    "# Calculate and print accuracy, classification report, and confusion matrix\n",
    "acc = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f'Accuracy: {acc}')\n",
    "\n",
    "# Calculate loss score on test set\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print test loss score\n",
    "print(f'Loss: {loss[0]}')\n",
    "\n",
    "\n",
    "\n",
    "# Save confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "\n",
    "\n",
    "# Log the confusion matrix to MLflow\n",
    "mlflow.log_artifact('confusion_matrix.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALLAN\\AppData\\Local\\Temp\\ipykernel_49516\\3544887439.py:9: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\ALLAN\\AppData\\Local\\Temp\\ipykernel_49516\\3544887439.py:20: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('training_and_validation_loss.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot accuracy from the loaded training history\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.savefig('training_and_validation_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "mlflow.log_artifact('training_and_validation_loss.png')\n",
    "mlflow.log_artifact('training_and_validation_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'ERROR': 0, 'INFO': 1, 'WARNING': 2}\n"
     ]
    }
   ],
   "source": [
    "# Print class names and their corresponding numerical values\n",
    "class_mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "print(\"Class mapping:\", class_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALLAN\\AppData\\Local\\Temp\\ipykernel_49516\\2957229105.py:14: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Get the last 100 real and predicted values\n",
    "y_test_last50 = y_test_labels[-200:]\n",
    "y_pred_last50 = y_pred_labels[-200:]\n",
    "\n",
    "# Create a line chart to visualize the real and predicted values\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(y_test_last50, label='Real', marker='o')\n",
    "plt.plot(y_pred_last50, label='Predicted', marker='x')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Criticality Events')\n",
    "plt.legend()\n",
    "plt.title('Comparison of Real and Predicted Criticality Events (Last 200)')\n",
    "plt.savefig('real_and_predicted.png')\n",
    "plt.show()\n",
    "\n",
    "mlflow.log_artifact('real_and_predicted.png')\n",
    "\n",
    "# End MLflow tracking\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the start and end dates for the next week\n",
    "# start_date = X_test_index.max() + datetime.timedelta(days=1)\n",
    "# end_date = start_date + datetime.timedelta(weeks=1)\n",
    "\n",
    "# # Generate a date range for the next week at 10-second intervals\n",
    "# next_week_dates = pd.date_range(start=start_date, end=end_date, freq='10S')[:-1]\n",
    "\n",
    "\n",
    "# # Create a new DataFrame with the index set as the 'created_at' column\n",
    "# X_test_with_index = pd.DataFrame(X_test.reshape(X_test.shape[0], -1), columns=X.columns, index=X_test_index)\n",
    "\n",
    "# # Merge the new DataFrame with the original data\n",
    "# X_test_resampled = X_test_with_index.reset_index().merge(pd.DataFrame(index=next_week_dates), left_on='created_at', right_index=True, how='outer')\n",
    "\n",
    "# # Forward fill the missing data\n",
    "# X_test_resampled.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# # Drop the 'created_at' column\n",
    "# X_test_resampled.drop(columns=['created_at'], inplace=True)\n",
    "\n",
    "# # Resample y_test\n",
    "# y_test_resampled = y_test.reset_index().merge(pd.DataFrame(index=next_week_dates), left_on='created_at', right_index=True, how='outer')\n",
    "# y_test_resampled['created_at'] = pd.to_datetime(y_test_resampled['created_at'])  # Add this line\n",
    "# y_test_resampled.fillna(method='ffill', inplace=True)\n",
    "# y_test_resampled.drop(columns=['created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict the 'criticality_events' for the resampled X_test data\n",
    "# X_test_resampled_3D = X_test_resampled.values.reshape(X_test_resampled.shape[0], 1, X_test_resampled.shape[1])\n",
    "# y_pred_resampled = model.predict(X_test_resampled_3D)\n",
    "\n",
    "# # Convert the predicted probabilities to class labels\n",
    "# y_pred_resampled_labels = np.argmax(y_pred_resampled, axis=1)\n",
    "# y_pred_resampled_labels = le.inverse_transform(y_pred_resampled_labels)\n",
    "\n",
    "# # Truncate or extend the index to match the length of the predicted values\n",
    "# if len(next_week_dates) > len(y_pred_resampled_labels):\n",
    "#     next_week_dates = next_week_dates[:len(y_pred_resampled_labels)]\n",
    "# elif len(next_week_dates) < len(y_pred_resampled_labels):\n",
    "#     next_week_dates = pd.date_range(start=start_date, periods=len(y_pred_resampled_labels), freq='10S')\n",
    "\n",
    "# # Create a DataFrame with predicted labels and timestamps\n",
    "# y_pred_resampled_df = pd.DataFrame(y_pred_resampled_labels, columns=['predicted_criticality'], index=next_week_dates)\n",
    "\n",
    "# # Merge the true labels with the predicted labels\n",
    "# comparison_df = y_test_resampled.reset_index().rename(columns={'index': 'created_at'})\n",
    "# comparison_df['predicted_criticality'] = y_pred_resampled_labels\n",
    "# comparison_df.set_index('created_at', inplace=True)\n",
    "\n",
    "# # Rename the columns for better readability\n",
    "# comparison_df.columns = ['true_criticality', 'predicted_criticality']\n",
    "\n",
    "# plt.figure(figsize=(20, 6))\n",
    "# plt.plot(comparison_df.index, comparison_df['true_criticality'], label='True')\n",
    "# plt.plot(comparison_df.index, comparison_df['predicted_criticality'], label='Predicted', linestyle='--', alpha=0.7)\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('Criticality Events')\n",
    "# plt.title('True vs. Predicted Criticality Events')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
