{"description": "Le meilleur mod\u00e8le de prediction de criticit\u00e9 du run est rf avec la metric choisi qui est l'accuracy qui est de 0.695 pour ce run", "params": {"memory": "None", "steps": "[('scaler', StandardScaler()), ('model', RandomForestClassifier(max_depth=20, max_features='log2', min_samples_split=10))]", "verbose": "False", "scaler": "StandardScaler()", "model": "RandomForestClassifier(max_depth=20, max_features='log2', min_samples_split=10)", "scaler__copy": "True", "scaler__with_mean": "True", "scaler__with_std": "True", "model__bootstrap": "True", "model__ccp_alpha": "0.0", "model__class_weight": "None", "model__criterion": "gini", "model__max_depth": "20", "model__max_features": "log2", "model__max_leaf_nodes": "None", "model__max_samples": "None", "model__min_impurity_decrease": "0.0", "model__min_samples_leaf": "1", "model__min_samples_split": "10", "model__min_weight_fraction_leaf": "0.0", "model__n_estimators": "100", "model__n_jobs": "None", "model__oob_score": "False", "model__random_state": "None", "model__verbose": "0", "model__warm_start": "False"}, "params_type": "{'lr': {'model': LogisticRegression(), 'params': {'model__C': [0.01, 0.1, 1.0, 10.0, 100.0], 'model__penalty': ['l2', 'none'], 'model__max_iter': [100, 200, 300]}}, 'rf': {'model': RandomForestClassifier(), 'params': {'model__n_estimators': [25, 50, 100, 150], 'model__max_depth': [None, 10, 20, 30], 'model__min_samples_split': [2, 5, 10], 'model__min_samples_leaf': [1, 2, 4], 'model__max_features': ['auto', 'sqrt', 'log2']}}, 'svc': {'model': SVC(probability=True), 'params': {'model__C': [0.01, 0.1, 1.0, 10.0, 100.0], 'model__gamma': [0.001, 0.01, 0.1, 1.0, 10.0]}}, 'xgb': {'model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...), 'params': {'model__n_estimators': [100, 200, 300, 400], 'model__learning_rate': [0.01, 0.05, 0.1, 0.2], 'model__max_depth': [3, 4, 5, 6]}}, 'lgbm': {'model': LGBMClassifier(), 'params': {'model__n_estimators': [100, 200, 300, 400], 'model__learning_rate': [0.01, 0.05, 0.1, 0.2], 'model__num_leaves': [31, 41, 51, 61]}}, 'dt': {'model': DecisionTreeClassifier(), 'params': {'model__criterion': ['gini', 'entropy'], 'model__max_depth': [None, 2, 4, 6, 8, 10], 'model__min_samples_split': [2, 5, 10], 'model__min_samples_leaf': [1, 2, 4]}}, 'gbc': {'model': GradientBoostingClassifier(), 'params': {'model__n_estimators': [100, 200, 300, 400], 'model__learning_rate': [0.01, 0.05, 0.1, 0.2], 'model__max_depth': [3, 5, 7]}}, 'nb': {'model': GaussianNB(), 'params': {'model__var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06]}}} (indique la cat\u00e9gorie de param\u00e8tres utilis\u00e9e : 'heavy', 'medium', 'light', ou 'test'. 'Heavy' implique un grand nombre de param\u00e8tres, 'medium' un nombre mod\u00e9r\u00e9, 'light' un nombre restreint, tandis que 'test' d\u00e9signe des mod\u00e8les et hyperparam\u00e8tres simplifi\u00e9s pour des tests rapides du pipeline)", "scaler used": "StandardScaler a \u00e9t\u00e9 utilis\u00e9 sur ce run pour standardiser les donn\u00e9es de type num\u00e9rique", "sampling_info": "Sous-\u00e9chantillonnage r\u00e9alis\u00e9 pour \u00e9quilibrer les classes. Distribution initiale : INFO: 8703, ERROR: 26231, NO_EVENT: 8502, WARNING: 4381. Chaque classe a \u00e9t\u00e9 r\u00e9duite \u00e0 4381 \u00e9chantillons, correspondant au nombre dans la classe 'WARNING'. Les classes 'INFO', 'NO_EVENT' et 'WARNING' ont \u00e9t\u00e9 fusionn\u00e9es en une classe 'FALSE' (pas d'erreur), et la classe 'ERROR' a \u00e9t\u00e9 renomm\u00e9e 'TRUE' (erreur), r\u00e9sultant en deux nouvelles classes plus \u00e9quilibr\u00e9es : 'TRUE': 22338 et 'FALSE': 18306.", "test_size_pourcentage": 0.15, "X_train_sampling": "40644", "y_train_sampling": "40644", "X_test_sampling": "7173", "y_test_sampling": "7173", "gridsearch_details": {"cv": 5, "scoring_metric": "precision_class_true"}, "learningcurve_details": {"cv": 5, "scoring_metric": "precision_class_true"}, "params_choice": "heavy", "miscellaneous note": ""}