{"description": "Le meilleur mod\u00e8le de prediction de criticit\u00e9 du run est gbc avec la metric choisi qui est l'accuracy qui est de 0.582 pour ce run", "params": {"memory": "None", "steps": "[('scaler', StandardScaler()), ('model', GradientBoostingClassifier(learning_rate=0.01, max_depth=1))]", "verbose": "False", "scaler": "StandardScaler()", "model": "GradientBoostingClassifier(learning_rate=0.01, max_depth=1)", "scaler__copy": "True", "scaler__with_mean": "True", "scaler__with_std": "True", "model__ccp_alpha": "0.0", "model__criterion": "friedman_mse", "model__init": "None", "model__learning_rate": "0.01", "model__loss": "log_loss", "model__max_depth": "1", "model__max_features": "None", "model__max_leaf_nodes": "None", "model__min_impurity_decrease": "0.0", "model__min_samples_leaf": "1", "model__min_samples_split": "2", "model__min_weight_fraction_leaf": "0.0", "model__n_estimators": "100", "model__n_iter_no_change": "None", "model__random_state": "None", "model__subsample": "1.0", "model__tol": "0.0001", "model__validation_fraction": "0.1", "model__verbose": "0", "model__warm_start": "False"}, "params_type": "{'lr': {'model': LogisticRegression(), 'params': {'model__C': [0.1, 1.0, 10.0]}}, 'rf': {'model': RandomForestClassifier(), 'params': {'model__n_estimators': [25, 50, 100], 'model__max_depth': [None, 10, 20], 'model__min_samples_split': [2, 5, 10], 'model__min_samples_leaf': [1, 2, 4]}}, 'svc': {'model': SVC(probability=True), 'params': {'model__C': [0.1, 1.0, 10.0], 'model__gamma': [0.1, 1.0, 10.0]}}, 'xgb': {'model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=None, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n              predictor=None, random_state=None, ...), 'params': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.1, 0.2]}}, 'lgbm': {'model': LGBMClassifier(), 'params': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.1, 0.2]}}, 'dt': {'model': DecisionTreeClassifier(), 'params': {'model__criterion': ['gini', 'entropy'], 'model__max_depth': [None, 2, 4, 6, 8, 10], 'model__min_samples_split': [2, 5, 10], 'model__min_samples_leaf': [1, 2, 4]}}, 'gbc': {'model': GradientBoostingClassifier(), 'params': {'model__n_estimators': [100, 200, 300], 'model__learning_rate': [0.01, 0.1, 0.2], 'model__max_depth': [1, 3, 5]}}, 'nb': {'model': GaussianNB(), 'params': {'model__var_smoothing': [1e-09, 1e-07]}}} (indique la cat\u00e9gorie de param\u00e8tres utilis\u00e9e : 'heavy', 'medium', 'light', ou 'test'. 'Heavy' implique un grand nombre de param\u00e8tres, 'medium' un nombre mod\u00e9r\u00e9, 'light' un nombre restreint, tandis que 'test' d\u00e9signe des mod\u00e8les et hyperparam\u00e8tres simplifi\u00e9s pour des tests rapides du pipeline)", "scaler used": "StandardScaler a \u00e9t\u00e9 utilis\u00e9 sur ce run pour standardiser les donn\u00e9es de type num\u00e9rique", "sampling_info": "Sur-\u00e9chantillonnage r\u00e9alis\u00e9 pour \u00e9quilibrer les classes. Distribution initiale : INFO: 26231, ERROR: 26231, NO_EVENT: 25557, WARNING: 12970. 2 classes ont \u00e9t\u00e9 augment\u00e9 \u00e0 12970 \u00e9chantillons, correspondant au nombre des classes majoritaire 'INFO' et 'ERROR'. Les classes 'INFO', 'NO_EVENT' et 'WARNING' ont \u00e9t\u00e9 fusionn\u00e9es en une classe 'FALSE' (pas d'erreur), et la classe 'ERROR' a \u00e9t\u00e9 renomm\u00e9e 'TRUE' (erreur), r\u00e9sultant en deux nouvelles classes \u00e9quilibr\u00e9es : 'TRUE': 22270 et 'FALSE': 55070 (55070 pour les 2 classes avec SMOTE pour le jeu d'entrainement)", "test_size_pourcentage": 0.11, "X_train_sampling": "110140", "y_train_sampling": "110140", "X_test_sampling": "13649", "y_test_sampling": "13649", "gridsearch_details": {"cv": 5, "scoring_metric": "precision_class_true"}, "learningcurve_details": {"cv": 5, "scoring_metric": "precision_class_true"}, "params_choice": "medium", "miscellaneous note": ""}