{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de création d'un csv jobs_events des données fractionnées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook génère 2 csv :\n",
    "\n",
    "- raw_merge_job_events_dataset.csv qui fusionne les données du dataset de brut (une ligne par job id)\n",
    "\n",
    "- raw_concat_job_events_dataset.csv qui concatène les données du dataset de brut (une ligne par tag)\n",
    "\n",
    "Etapes : \n",
    "\n",
    "- fractionnement de la colonne payload\n",
    "\n",
    "- fractionnement des sous-colonnes\n",
    "\n",
    "- fusion des sous-colonnes entre elle (chaque job à une ligne et regroupe les données des 3 tags)\n",
    "\n",
    "- concaténation des sous-colonnes (chaque job à plusieurs ligne : job_start, job_preview, job_end)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction retournant un dataframe à partir du payload pour un tag ciblé\n",
    "def payload_dataframe_by_tag(input_df, tag):\n",
    "    # creation du dataframe avec selection par tag\n",
    "    df = input_df.loc[input_df['tag'] == tag]\n",
    "    # creation du dataframe du payload fractionné\n",
    "    payload_df = df.payload.apply(lambda x : json.loads(x)).apply(pd.Series)\n",
    "    # merge des 2 dataframes\n",
    "    tag_df = df.merge(payload_df,left_index=True, right_index=True)\n",
    "    # suppression de la colonne 'payload' et de la colonne 'tag'\n",
    "    tag_df.drop(['payload','tag'], axis=1, inplace=True)\n",
    "    # remise à 0 des index\n",
    "    tag_df.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='')\n",
    "    return tag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction retournant le dataframe d'une colonne fractionnée\n",
    "# col=colonne à fractionner\n",
    "# df=dataframe source\n",
    "# data=dict des colonnes du df à conserver dans le df à retourner\n",
    "def convert_col_to_df(col:str, df:pd.DataFrame, data:dict=None):\n",
    "    \n",
    "    # création du dictionnaire de données vide\n",
    "    if data == None :\n",
    "        data = {}\n",
    "    # ou liste des clés du dictionnaire input\n",
    "    else :\n",
    "        data_keys = list(data.keys())\n",
    "\n",
    "    # on converti le type des valeurs str en list\n",
    "    if not isinstance(df[col].loc[0], list) and not isinstance(df[col].loc[0], dict):\n",
    "        try :\n",
    "            df[col] = df[col].apply(lambda x : json.loads(x))\n",
    "        except:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    # liste des clés du dictionnaire de la colonne à partir de la première occurence\n",
    "    # on recherche la première occurence non vide et de type list \n",
    "    # pour l'affecter à une variable first\n",
    "    for i in range(0, (len(df[col]))):\n",
    "        value = df[col].loc[i]\n",
    "        if isinstance(value, list):\n",
    "            if len(value) > 0 :\n",
    "                first = value[0]\n",
    "                #print('first: ', type(first), first)\n",
    "                break\n",
    "        if isinstance(value, dict):\n",
    "            if len(value) > 0 :\n",
    "                first = value\n",
    "                #print('first: ', type(first), first)\n",
    "                break\n",
    "\n",
    "    # on liste les clés du dictionnaire de l'occurence\n",
    "    col_keys = first.keys()\n",
    "    for ck in col_keys :\n",
    "        data[ck+'_'+col] = []\n",
    "    # print(data)\n",
    "    \n",
    "    # on itére dans la serie pour récupérer les valeurs et les stocker dans le dictionnaire data\n",
    "    for i in range(df.index.start, df.index.stop):\n",
    "        # evaluation des valeurs 'str' en 'list'\n",
    "        values = df[col].loc[i]\n",
    "        if isinstance(values, list) and len(values) > 0 :\n",
    "            # ajout des valeurs dans le dictionnaire 'd'\n",
    "            for value in values :\n",
    "                for k in value.keys():\n",
    "                    data[k+'_'+col].append(value.get(k))\n",
    "                for dk in data_keys:\n",
    "                    data[dk].append(df[dk].loc[i])\n",
    "        if isinstance(values, dict) :\n",
    "            # ajout des valeurs dans le dictionnaire 'd'\n",
    "            for k in values.keys() :\n",
    "                data[k+'_'+col].append(values.get(k))\n",
    "            for dk in data_keys:\n",
    "                data[dk].append(df[dk].loc[i])\n",
    "                \n",
    "    # re-assignation de la variable df\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source path to raw metrics dataset\n",
    "filename = 'job_events.csv'\n",
    "path = '../data/raw/'\n",
    "# target path to save merge raw job events dataset\n",
    "save_csv_merge = '../data/jobs/raw_merge_job_events_dataset.csv'\n",
    "save_csv_concat = '../data/jobs/raw_concat_job_events_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# téléchargement dans le repertoire 'data' d'un fichiers 'csv' depuis le blob\n",
    "from azure_blob import download_blob_file\n",
    "download_blob_file(file_name=filename, local_path=path)\n",
    "job_events = os.path.join(path, filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Création"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dataframe à partir du csv de données\n",
    "job_events_df = pd.read_csv(job_events).sort_values(by='received_at')\n",
    "job_events_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# réindexation\n",
    "job_events_df.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='')\n",
    "job_events_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on verifie que les valeurs de la colonne id n'ont pas de doublon\n",
    "any(job_events_df.id.duplicated())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Fractionnement du payload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu du payload diffère selon le tag donc on subdivise le dataset en fonction du tag pour fractionner le payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des tag\n",
    "job_events_df.tag.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation des dataframes du payload fractionné pour chaque tag\n",
    "job_event_payload = job_events_df.drop(['machine_id','received_at'], axis=1).copy()\n",
    "job_started_df = payload_dataframe_by_tag(input_df=job_event_payload, tag='job-started')\n",
    "job_preview_df = payload_dataframe_by_tag(input_df=job_event_payload, tag='job-preview-ready')\n",
    "job_ended_df = payload_dataframe_by_tag(input_df=job_event_payload, tag='job-ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on verifie que les valeurs de la colonne jobId soient unique dans chaque df\n",
    "print('job-started tag :', any(job_started_df.jobId.duplicated()), job_started_df.jobId.nunique())\n",
    "print('job-preview tag :', any(job_preview_df.jobId.duplicated()), job_preview_df.jobId.nunique())\n",
    "print('job-ended tag :', any(job_ended_df.jobId.duplicated()), job_ended_df.jobId.nunique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dataframe de tag job start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des valeurs\n",
    "job_started_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes ne contenant aucune valeurs :\n",
    "job_started_df = job_started_df.drop(['memjet','octopus'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des colonnes contenant des valeurs de type list ou dict à fractionner\n",
    "job_started_col_to_split = []\n",
    "for col in job_started_df.columns :\n",
    "    if isinstance(job_started_df[col].loc[0], list) or isinstance(job_started_df[col].loc[0], dict):\n",
    "        job_started_col_to_split.append(col)\n",
    "\n",
    "job_started_col_to_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Fractionnement colonne iper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_started_df.iper.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "iper = convert_col_to_df('iper', job_started_df, {'id':[]})\n",
    "iper.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de colonne\n",
    "#iper = iper.drop(['bars_iper'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "iper_col_to_drop = []\n",
    "for col in iper.drop(['bars_iper'], axis=1).columns:\n",
    "    print(col, iper[col].nunique())\n",
    "    if iper[col].nunique() <= 1 :\n",
    "        iper_col_to_drop.append(col)\n",
    "col_to_drop.append(iper_col_to_drop)\n",
    "iper_col_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Fractionnement colonne user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les valeurs\n",
    "job_started_df.user.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "user = convert_col_to_df('user', job_started_df, {'id':[]})\n",
    "user.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Fractionnement colonne ifoil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les valeurs\n",
    "job_started_df.ifoil.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "ifoil = convert_col_to_df('ifoil', job_started_df, {'id':[]})\n",
    "ifoil.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "ifoil_col_to_drop = []\n",
    "for col in ifoil.drop(['stampAreas_ifoil'], axis=1).columns:\n",
    "    print(col, ifoil[col].nunique())\n",
    "    if ifoil[col].nunique() <= 1 :\n",
    "        ifoil_col_to_drop.append(col)\n",
    "col_to_drop.append(ifoil_col_to_drop)\n",
    "ifoil_col_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Fractionnement colonne layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les valeurs\n",
    "job_started_df.layout.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "layout_df = convert_col_to_df('layout', job_started_df, {'id':[]})\n",
    "layout_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "imageLayout_layout = convert_col_to_df('imageLayout_layout', layout_df, {'id':[]})\n",
    "imageLayout_layout.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "paperFormat_layout = convert_col_to_df('paperFormat_layout', layout_df, {'id':[]})\n",
    "paperFormat_layout.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fusionne les colonnes fractionnées\n",
    "merge_imageLayout_paperFormat = pd.merge(imageLayout_layout, paperFormat_layout, how='outer', on='id')\n",
    "merge_layout = pd.merge(merge_imageLayout_paperFormat, layout_df, how='outer', on='id')\n",
    "merge_layout = merge_layout.drop(['imageLayout_layout','paperFormat_layout'], axis=1)\n",
    "merge_layout.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "layout_col_to_drop = []\n",
    "for col in merge_layout.columns:\n",
    "    print(col, merge_layout[col].nunique())\n",
    "    if merge_layout[col].nunique() <= 1 :\n",
    "        layout_col_to_drop.append(col)\n",
    "col_to_drop.append(layout_col_to_drop)\n",
    "layout_col_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Fractionnement colonne irDryers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_started_df.irDryers.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "irDryers = convert_col_to_df('irDryers', job_started_df, {'id':[]})\n",
    "irDryers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "irDryers_col_to_drop = []\n",
    "for col in irDryers.columns:\n",
    "    print(col, irDryers[col].nunique())\n",
    "    if irDryers[col].nunique() <= 1 :\n",
    "        irDryers_col_to_drop.append(col)\n",
    "col_to_drop.append(irDryers_col_to_drop)\n",
    "irDryers_col_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Fractionnement colonne uvDryers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_started_df.uvDryers.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "uvDryers = convert_col_to_df('uvDryers', job_started_df, {'id':[]})\n",
    "uvDryers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "uvDryers_col_to_drop = []\n",
    "for col in uvDryers.columns:\n",
    "    print(col, uvDryers[col].nunique())\n",
    "    if uvDryers[col].nunique() <= 1 :\n",
    "        uvDryers_col_to_drop.append(col)\n",
    "col_to_drop.append(uvDryers_col_to_drop)\n",
    "uvDryers_col_to_drop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Fractionnement colonne remoteScannerRegistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_started_df.remoteScannerRegistration.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne la colonne\n",
    "remoteScannerRegistration = convert_col_to_df('remoteScannerRegistration', job_started_df, {'id':[]})\n",
    "remoteScannerRegistration.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on liste les sous-colonnes à fractionner, leurs valeurs sont de type list ou dict:\n",
    "remoteScannerRegistration_col_to_split = []\n",
    "for col in remoteScannerRegistration.columns:\n",
    "    if isinstance(remoteScannerRegistration[col].loc[0], list) or isinstance(remoteScannerRegistration[col].loc[0], dict):\n",
    "        remoteScannerRegistration_col_to_split.append(col)\n",
    "\n",
    "remoteScannerRegistration_col_to_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration > gridMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "gridMode = convert_col_to_df('gridMode_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "gridMode.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "descriptor = convert_col_to_df('descriptor_gridMode_remoteScannerRegistration', gridMode, {'id':[]})\n",
    "descriptor.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fusionne les colonnes fractionnées\n",
    "merge_gridMode = pd.merge(gridMode, descriptor, how='outer', on='id')\n",
    "merge_gridMode = merge_gridMode.drop(['descriptor_gridMode_remoteScannerRegistration'], axis=1)\n",
    "merge_gridMode.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration > registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "registration = convert_col_to_df('registration_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "registration.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration > cropmarksMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "cropmarksMode = convert_col_to_df('cropmarksMode_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "cropmarksMode.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropmarksMode_1 = convert_col_to_df('cropmark1_cropmarksMode_remoteScannerRegistration', cropmarksMode, {'id':[]})\n",
    "cropmarksMode_2 = convert_col_to_df('cropmark2_cropmarksMode_remoteScannerRegistration', cropmarksMode, {'id':[]})\n",
    "merge_cropmarksModes = pd.merge(cropmarksMode_1, cropmarksMode_2, how='outer', on='id')\n",
    "merge_cropmarksMode = pd.merge(cropmarksMode, merge_cropmarksModes, how='outer', on='id')\n",
    "merge_cropmarksMode = merge_cropmarksMode.drop(['cropmark1_cropmarksMode_remoteScannerRegistration','cropmark2_cropmarksMode_remoteScannerRegistration'], axis=1)\n",
    "merge_cropmarksMode.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration > manualLighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "manualLighting = convert_col_to_df('manualLighting_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "manualLighting.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platePoint = convert_col_to_df('platePoint_manualLighting_remoteScannerRegistration', manualLighting, {'id':[]})\n",
    "substratePoint = convert_col_to_df('substratePoint_manualLighting_remoteScannerRegistration', manualLighting, {'id':[]})\n",
    "merge_plate_substrate = pd.merge(platePoint, substratePoint, how='outer', on='id')\n",
    "merge_manualLighting = pd.merge(manualLighting, merge_plate_substrate, how='outer', on='id')\n",
    "merge_manualLighting = merge_manualLighting.drop(['platePoint_manualLighting_remoteScannerRegistration','substratePoint_manualLighting_remoteScannerRegistration'], axis=1)\n",
    "merge_manualLighting.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration > fullScannerMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "fullScannerMode = convert_col_to_df('fullScannerMode_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "fullScannerMode.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### remoteScannerRegistration >  specialSubstrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne une colonne\n",
    "specialSubstrate = convert_col_to_df('specialSubstrate_remoteScannerRegistration', remoteScannerRegistration, {'id':[]})\n",
    "specialSubstrate.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### fusion des sous-colonnes remoteScannerRegistration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_registration_gridMode_df = pd.merge(registration, merge_gridMode, how='outer', on='id')\n",
    "merge_cropmarksMode_df = pd.merge(merge_registration_gridMode_df, merge_cropmarksMode, how='outer', on='id')\n",
    "merge_manualLighting_df = pd.merge(merge_cropmarksMode_df, merge_manualLighting, how='outer', on='id')\n",
    "merge_fullScannerMode_df = pd.merge(merge_manualLighting_df, fullScannerMode, how='outer', on='id')\n",
    "merge_specialSubstrate_df = pd.merge(merge_fullScannerMode_df, specialSubstrate, how='outer', on='id')\n",
    "merge_remoteScannerRegistration = pd.merge(merge_specialSubstrate_df, remoteScannerRegistration, how='outer', on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des sous-colonnes qui ont été fractionnées\n",
    "merge_remoteScannerRegistration = merge_remoteScannerRegistration.drop(remoteScannerRegistration_col_to_split, axis=1)\n",
    "merge_remoteScannerRegistration.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### suppression des colonnes contenant des valeurs nulles ou une valeur unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "remoteScannerRegistration_df_col_to_drop = []\n",
    "for col in merge_remoteScannerRegistration.columns:\n",
    "    #print(col, merge_remoteScannerRegistration_df[col].nunique())\n",
    "    if merge_remoteScannerRegistration[col].nunique() <= 1 :\n",
    "        remoteScannerRegistration_df_col_to_drop.append(col)\n",
    "\n",
    "print('nombre total de colonnes :', merge_remoteScannerRegistration.shape[1])\n",
    "print('nombre de colonnes à supprimer :', len(remoteScannerRegistration_df_col_to_drop))\n",
    "remoteScannerRegistration_df_col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes\n",
    "merge_remoteScannerRegistration = merge_remoteScannerRegistration.drop(remoteScannerRegistration_df_col_to_drop, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Fusion des colonnes fractionnées de job_started_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_iper_user = pd.merge(iper, user, how='outer', on='id')\n",
    "merge_ifoil_df = pd.merge(merge_iper_user, ifoil, how='outer', on='id')\n",
    "merge_layout_df = pd.merge(merge_ifoil_df, merge_layout, how='outer', on='id')\n",
    "merge_irDryers_df = pd.merge(merge_layout_df, irDryers, how='outer', on='id')\n",
    "merge_uvDryers_df = pd.merge(merge_irDryers_df, uvDryers, how='outer', on='id')\n",
    "merge_remoteScannerRegistration_df = pd.merge(merge_uvDryers_df, merge_remoteScannerRegistration, how='outer', on='id')\n",
    "merge_remoteScannerRegistration_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes contenant des valeurs uniques\n",
    "for cols in col_to_drop :\n",
    "    for col in cols :\n",
    "        merge_remoteScannerRegistration_df = merge_remoteScannerRegistration_df.drop(col, axis=1)\n",
    "merge_remoteScannerRegistration_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_job_started_df = pd.merge(job_started_df, merge_remoteScannerRegistration_df, how='outer', on='id')\n",
    "merge_job_started_df = merge_job_started_df.drop(job_started_col_to_split, axis=1)\n",
    "merge_job_started_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Dataframe de tag job preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les données\n",
    "job_preview_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # on visualise l'image\n",
    "# import PIL.Image as Image\n",
    "# import io, base64\n",
    "# byte_data = job_preview_df.image.loc[0]\n",
    "# b = base64.b64decode(byte_data)\n",
    "# img = Image.open(io.BytesIO(b))\n",
    "# img.show()\n",
    "# img_name = job_preview_df.path.loc[0].split('/')[-1]\n",
    "# img.save(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime la colonne machineId\n",
    "job_preview_df = job_preview_df.drop(['machineId'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces données sont utiles pour afficher les images des job mais pas pertinentes pour l'exploration ou la prédiction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Dataframe de tag job end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les données\n",
    "job_ended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime la colonne machineId\n",
    "job_ended_df = job_ended_df.drop(['machineId'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne varnishConsumption est fractionnable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fractionnement colonne 'varnishConsumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ended_df.varnishConsumption.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnishConsumption = convert_col_to_df('varnishConsumption', job_ended_df, {'id':[]})\n",
    "varnishConsumption.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fractionnement colonne 'operatorSideTanks_varnishConsumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operatorSideTanks = convert_col_to_df('operatorSideTanks_varnishConsumption', varnishConsumption, {'id':[]})\n",
    "operatorSideTanks.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Suppression colonne 'technicalSideTanks_varnishConsumption'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne ne contient aucune valeur\n",
    "varnishConsumption = varnishConsumption.drop(['technicalSideTanks_varnishConsumption'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fusion des colonnes varnishConsumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_varnishConsumption = pd.merge(varnishConsumption, operatorSideTanks, how='outer', on='id')\n",
    "merge_varnishConsumption = merge_varnishConsumption.drop(['operatorSideTanks_varnishConsumption'], axis=1)\n",
    "merge_varnishConsumption.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fusion des sous-colonnes job_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_job_ended_df = pd.merge(job_ended_df, merge_varnishConsumption, how='outer', on='id')\n",
    "merge_job_ended_df = merge_job_ended_df.drop(['varnishConsumption'], axis=1)\n",
    "merge_job_ended_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste le nombre de valeurs uniques par colonne\n",
    "merge_job_ended_df_col_to_drop = []\n",
    "for col in merge_job_ended_df.columns:\n",
    "    if merge_job_ended_df[col].nunique() <= 1 :\n",
    "        merge_job_ended_df_col_to_drop.append(col)\n",
    "\n",
    "print('nombre total de colonnes :', merge_job_ended_df.shape[1])\n",
    "print('nombre de colonnes à supprimer :', len(merge_job_ended_df_col_to_drop))\n",
    "merge_job_ended_df_col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes à valeur unique\n",
    "merge_job_ended_df = merge_job_ended_df.drop(merge_job_ended_df_col_to_drop, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Creation du dataframe final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vérification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation des dataframes du payload\n",
    "print('job_started shape :', merge_job_started_df.shape)\n",
    "print('job_preview shape :', job_preview_df.shape)\n",
    "print('job_ended shape :', merge_job_ended_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on verifie l'intégrité des jobId\n",
    "print(merge_job_started_df['jobId'].isin(job_started_df['jobId']).value_counts())\n",
    "print(merge_job_ended_df['jobId'].isin(job_ended_df['jobId']).value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a toujours autant de jobId dans les df fusionnés et les df de départ ne comportaient aucun doublon.\n",
    "\n",
    "On peut donc les fusionner sur la valeur de jobId."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Par fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fusionnes les datasets des tag start et tag end par job id\n",
    "merge_start_end_df = pd.merge(\n",
    "    merge_job_started_df.drop(['id'],axis=1), \n",
    "    merge_job_ended_df.drop(['id'],axis=1), \n",
    "    how='outer', \n",
    "    on='jobId',\n",
    "    suffixes=['_start', '_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_payload_df = pd.merge(\n",
    "    merge_start_end_df, \n",
    "    job_preview_df.drop(['id'],axis=1), \n",
    "    how='outer', \n",
    "    on='jobId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_payload_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_payload_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_payload_df.to_csv(save_csv_merge)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Par concaténation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'on souhaite conserver un dataset avec un tag par ligne on effectue une concaténation des datasets tag start et end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on concatene les dataset des tag start et tag end\n",
    "concat_job_events_df = pd.concat([merge_job_started_df,merge_job_ended_df])\n",
    "concat_job_events_df.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(concat_job_events_df.info())\n",
    "concat_job_events_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_job_events_df.to_csv(save_csv_concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2315c9af7dedaeb0b2bf51504304a927c605b523f04dad98936c50abe500b408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
