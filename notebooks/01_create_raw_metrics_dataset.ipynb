{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Creation de merge raw metrics dataset "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook génère :\n",
    "\n",
    "- 1 fichier csv \"merge_raw_metrics_dataset.csv\" qui fusionne les colonnes fractionnées avec le dataset d'origine\n",
    "- 1 fichier json \"metrics_events_dict.json\" pour lister les code d'identification des évènements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from azure_blob import download_blob_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Création dataset metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source path to raw metrics dataset\n",
    "filename = 'metrics.csv'\n",
    "path = '../data/'\n",
    "source_csv = os.path.join(path, filename)\n",
    "# target path to save metrics dictionnaire\n",
    "save_json ='../data/metrics/metrics_events_dict.json'\n",
    "# target path to save merge raw metrics dataset\n",
    "save_csv = '../data/metrics/merge_raw_metrics_dataset.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import des données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# téléchargement dans le repertoire 'data' d'un fichiers 'csv' depuis le blob Azure\n",
    "download_blob_file(file_name=filename, local_path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dataframe à partir du csv de données\n",
    "metrics_df = pd.read_csv(filepath_or_buffer=Path(source_csv)).sort_values(by='created_at')\n",
    "metrics_df.reset_index(level=None, drop=True, inplace=True, col_level=0, col_fill='')\n",
    "metrics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des colonnes ne contenant aucune valeurs\n",
    "metrics_df = metrics_df.dropna(axis=1)\n",
    "# suppression de la colonne machineId\n",
    "metrics_df = metrics_df.drop('machineId', axis=1)\n",
    "# visualisation des 3 premières lignes\n",
    "metrics_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Fractionnement des colonnes contenant des listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on verifie le type des valeurs contenu dans les colonnes de type objet contenant des listes\n",
    "print('modules :', type(metrics_df.modules.loc[0]))\n",
    "print('events :', type(metrics_df.events.loc[0]))\n",
    "print('connected_operators :', type(metrics_df.connected_operators.loc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction retournant le dataframe d'une colonne fractionnée\n",
    "# col=colonne à fractionner\n",
    "# df=dataframe source\n",
    "# data=dict des colonnes du df à conserver dans le df à retourner\n",
    "def convert_col_to_df(col, df, data=None):\n",
    "    \n",
    "    # création du dictionnaire de données vide\n",
    "    if data == None :\n",
    "        data = {}\n",
    "    # ou liste des clés du dictionnaire input\n",
    "    else :\n",
    "        data_keys = list(data.keys())\n",
    "\n",
    "    # on converti le type des valeurs str en list\n",
    "    if not isinstance(df[col].loc[0], list):\n",
    "        df[col] = df[col].apply(lambda x : json.loads(x))\n",
    "\n",
    "    # liste des clés du dictionnaire de la colonne à partir de la première occurence\n",
    "    # on recherche la première occurence non vide et de type list \n",
    "    # pour l'affecter à une variable first\n",
    "    for i in range(0, (len(df[col]))):\n",
    "        value = df[col].loc[i]\n",
    "        if len(value) > 0 and isinstance(value, list):\n",
    "            first = value[0]\n",
    "            print('first : ', type(first), first)\n",
    "            break\n",
    "\n",
    "    # on liste les clés du dictionnaire de l'occurence\n",
    "    col_keys = first.keys()\n",
    "    for ck in col_keys :\n",
    "        data[ck] = []\n",
    "\n",
    "    # on itére dans la serie pour récupérer les valeurs et les stocker dans le dictionnaire data\n",
    "    for i in range(df.index.start, df.index.stop):\n",
    "        # evaluation des valeurs 'str' en 'list'\n",
    "        values = df[col].loc[i]\n",
    "        if isinstance(values, list) and len(values) > 0 :\n",
    "            # ajout des valeurs dans le dictionnaire 'd'\n",
    "            for value in values :\n",
    "                for k in value.keys():\n",
    "                    data[k].append(value.get(k))\n",
    "                for dk in data_keys:\n",
    "                    data[dk].append(df[dk].loc[i])\n",
    "\n",
    "    # re-assignation de la variable df\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id temoin pour vérifier le fractionnement et la fusion des colonnes\n",
    "check_id = 4170152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# détail de la ligne témoin\n",
    "check_line = metrics_df[metrics_df.id == check_id]\n",
    "print('######## id %d ######## ' %check_id)\n",
    "print(check_line.values)\n",
    "print('######## id %d modules details ######## ' %check_id)\n",
    "print(json.loads(check_line.modules.values[0])[0].get('counters'))\n",
    "print(json.loads(check_line.modules.values[0])[1].get('counters'))\n",
    "print('######## id %d events details ######## ' %check_id)\n",
    "print(json.loads(check_line.events.values[0])[0])\n",
    "print(json.loads(check_line.events.values[0])[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Colonne 'connected_operators'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe connected_operators (~42s)\n",
    "connected_operators_df = convert_col_to_df('connected_operators', metrics_df, {'id':[]})\n",
    "print(connected_operators_df.info())\n",
    "connected_operators_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Colonne 'events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe events (~20s)\n",
    "events_df = convert_col_to_df('events', metrics_df, {'id':[]})\n",
    "print(events_df.info())\n",
    "events_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des codes d'identification\n",
    "identification_codes_list = events_df['identification'].unique()\n",
    "np.sort(identification_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des évènements uniques\n",
    "identification_dict = {}\n",
    "c = 1\n",
    "id_list = []\n",
    "for i in range(events_df.index.start, events_df.index.stop):\n",
    "    id = events_df.identification.loc[i]\n",
    "    if id not in id_list:\n",
    "        id_list.append(id)\n",
    "        identification_dict[id] = events_df.message.loc[i]\n",
    "        c += 1\n",
    "identification_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des sources\n",
    "source_list = events_df['source'].unique()\n",
    "np.sort(source_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des sources\n",
    "criticality_list = events_df['criticality'].unique()\n",
    "np.sort(criticality_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Events Json dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sauvegarde la liste des codes d'identification d'event\n",
    "with open(file=Path(save_json), mode=\"r+\", encoding='utf-8') as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    data['identification'] = identification_dict\n",
    "    data['criticality'] = list(np.sort(criticality_list)),\n",
    "    data['source'] = list(np.sort(source_list))\n",
    "    jsonFile.seek(0)\n",
    "    json.dump(data, jsonFile, indent=4, ensure_ascii=False)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Colonne 'modules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe modules (~1m15s)\n",
    "modules_df = convert_col_to_df('modules', metrics_df, {'id':[]})\n",
    "print(modules_df.info())\n",
    "modules_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Colonne counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe counters (~2m7s)\n",
    "counters_df = convert_col_to_df('counters', modules_df, {'type':[], 'id': []})\n",
    "print(counters_df.info())\n",
    "counters_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_check_line = counters_df[counters_df.id == check_id]\n",
    "counters_check_line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Fusion des dataframes des colonnes fractionnées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Merge modules et counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_modules_df = pd.merge(modules_df, counters_df, on=['id','type'], suffixes=['','_counter'])\n",
    "merge_modules_df = merge_modules_df.drop(['counters'], axis=1)\n",
    "module_check_line = merge_modules_df[merge_modules_df.id == check_id]\n",
    "module_check_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_modules_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Merge operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_operators_df = pd.merge(merge_modules_df, connected_operators_df, on='id', suffixes=['','_op'])\n",
    "op_check_line = merge_operators_df[merge_operators_df.id == check_id]\n",
    "op_check_line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Merge events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_events_df = pd.merge(merge_operators_df, events_df, how='outer', on='id', suffixes=['','_event'])\n",
    "events_check_line = merge_events_df[merge_events_df.id == check_id]\n",
    "events_check_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_events_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Merge metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_metrics_df = pd.merge(merge_events_df, metrics_df, how='outer', on='id', suffixes=['','_metrics'])\n",
    "merge_metrics_df = merge_metrics_df.drop(['status','connected_operators','modules','events'], axis=1)\n",
    "metrics_check_line = merge_metrics_df[merge_metrics_df.id == check_id]\n",
    "metrics_check_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_metrics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_metrics_df.to_csv(path_or_buf=Path(save_csv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2315c9af7dedaeb0b2bf51504304a927c605b523f04dad98936c50abe500b408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
