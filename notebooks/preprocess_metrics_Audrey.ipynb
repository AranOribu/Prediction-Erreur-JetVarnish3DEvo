{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook exploration données : metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook génère 2 fichiers csv \n",
    "\n",
    "- complet avec les données métrics nettoyées et les colonnes json fractionnées,\n",
    "\n",
    "- partiel avec les données métrics et ne conservant que les id des events."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from azure_blob import download_blob_file\n",
    "from preprocess_functions import convert_str_to_list, get_keys, add_key_column, convert_str_to_int_list\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'metrics.csv'\n",
    "path = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# téléchargement dans le repertoire 'data' d'un fichiers 'csv' depuis le blob\n",
    "download_blob_file(file_name=filename, local_path=path)\n",
    "metrics = os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dataframe à partir du csv de données\n",
    "metrics_df = pd.read_csv(metrics).sort_values(by='created_at')\n",
    "metrics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.dropna(axis=1, inplace=True)\n",
    "metrics_df.drop('machineId', axis=1, inplace=True)\n",
    "# metrics_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column \"connected_operators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics_df.connected_operators = metrics_df.connected_operators.apply(lambda x :json.loads(x)[0])\n",
    "connected_operators_df = convert_str_to_list(metrics_df.connected_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère les clés du dictionnaire dans la première ligne du dataset 'loc.[0]' dans la première case de la liste '[0]'\n",
    "connected_operators_keys = get_keys(connected_operators_df.connected_operators.loc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ajoute des colonnes pour chaque clés\n",
    "for key in connected_operators_keys:\n",
    "    connected_operators_df = add_key_column(connected_operators_df, 'connected_operators', connected_operators_keys,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime la colonne d'origine\n",
    "connected_operators_df.drop('connected_operators', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_operators_df.rename(\n",
    "    columns={\n",
    "    'connected_operators_name_0': 'operators_name',\n",
    "    'connected_operators_level_0': 'operators_level'\n",
    "    }, \n",
    "    inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connected_operators_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column \"modules\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df = convert_str_to_list(metrics_df.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_keys = get_keys(modules_df.modules.loc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in modules_keys:\n",
    "    modules_df = add_key_column(modules_df, 'modules', modules_keys,0)\n",
    "for key in modules_keys:\n",
    "    modules_df = add_key_column(modules_df, 'modules', modules_keys,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.drop('modules', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column \"modules counters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_counters_keys = get_keys(modules_df.modules_counters_0.loc[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in modules_counters_keys:\n",
    "    modules_df = add_key_column(modules_df, 'modules_counters_0', modules_counters_keys,0)\n",
    "for key in modules_counters_keys:\n",
    "    modules_df = add_key_column(modules_df, 'modules_counters_1', modules_counters_keys,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.drop(['modules_counters_0', 'modules_counters_1'], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.dropna(axis='columns', how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in modules_df.columns :\n",
    "    if modules_df[col].nunique() > 1 :\n",
    "        print(col, 'nombre de valeurs unique : %d' % modules_df[col].nunique())\n",
    "    else :\n",
    "        print(col, modules_df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = []\n",
    "for col in modules_df.columns :\n",
    "    if modules_df[col].nunique() == 1 :\n",
    "        if modules_df[col].unique().item() != '':\n",
    "            columns_to_drop.append(col)\n",
    "        else :\n",
    "            modules_df.drop(columns=[col], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.rename(columns={\n",
    "    'modules_counters_0_value_0': 'VarnishPrinter_3DVarnishCounter',\n",
    "    'modules_counters_1_value_0': 'iFoil_TotalPagesCounter'\n",
    "    }, inplace=True)\n",
    "modules_df.drop(columns=columns_to_drop, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column \"events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion des valeurs \"events\" string en list\n",
    "events_df = convert_str_to_list(metrics_df.events)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split events in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dataframe de la colonne events fractionnées\n",
    "import ast\n",
    "d = {'source': [],\n",
    "    'message': [],\n",
    "    'timestamp': [],\n",
    "    'criticality': [],\n",
    "    'identification': []}\n",
    "for values in events_df.events.values :\n",
    "    # pour chaque list non nulle\n",
    "    if len(values) > 0 :\n",
    "        # ajout des valeurs dans le dictionnaire 'd'\n",
    "        for event in values :\n",
    "            d['source'].append(event.get('source'))\n",
    "            d['message'].append(event.get('message'))\n",
    "            d['timestamp'].append(event.get('timestamp'))\n",
    "            d['criticality'].append(event.get('criticality'))\n",
    "            d['identification'].append(event.get('identification'))\n",
    "df =  pd.DataFrame(data=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des codes d'identification\n",
    "identification_codes_list = df['identification'].unique()\n",
    "np.sort(identification_codes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des évènements uniques\n",
    "identification_dict = {}\n",
    "c = 1\n",
    "id_list = []\n",
    "for i in range(df.index.start, df.index.stop):\n",
    "    id = df.identification.loc[i]\n",
    "    if id not in id_list:\n",
    "        id_list.append(id)\n",
    "        identification_dict[id] = df.message.loc[i]\n",
    "        c += 1\n",
    "identification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des sources\n",
    "source_list = df['source'].unique()\n",
    "np.sort(source_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste des sources\n",
    "criticality_list = df['criticality'].unique()\n",
    "np.sort(criticality_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on sauvegarde la liste des codes d'identification d'event\n",
    "from pathlib import Path\n",
    "filepath = Path('../data/metrics/metrics_events_dict.json')\n",
    "with open(file=filepath, mode=\"r+\", encoding='utf-8') as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "    data['identification'] = identification_dict\n",
    "    data['criticality'] = list(np.sort(criticality_list)),\n",
    "    data['source'] = list(np.sort(source_list))\n",
    "    jsonFile.seek(0)\n",
    "    json.dump(data, jsonFile, indent=4, ensure_ascii=False)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on liste les clés d'un dictionnaire d'event\n",
    "events_keys = []\n",
    "for i in range(0, len(events_df)) :\n",
    "    if len(events_df.events.loc[i]) != 0 :\n",
    "        event_keys = events_df.events.loc[i][0].keys()\n",
    "        if event_keys not in events_keys :\n",
    "            events_keys.append(event_keys)\n",
    "# clés d'un dictionnaire d'event\n",
    "events_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création d'un dictionnaire d'event vide\n",
    "events_dict = {}\n",
    "for key in event_keys :\n",
    "    events_dict[key] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout d'une colonne \"Length\" du nombre d'event pas ligne\n",
    "events_df['Length'] = events_df.events.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fractionnement des events\n",
    "events_split_df = pd.DataFrame(events_df.events.to_list(), dtype='object', index=events_df.index)\n",
    "events_split_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplacement des valeurs \"None\" par des dictionnaires \n",
    "for col in events_split_df.columns :\n",
    "    for i in range(0,len(events_split_df.iloc[:,col])):\n",
    "        if isinstance(events_split_df.loc[i][col], type(None)) :\n",
    "            events_split_df.loc[i][col] = events_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renommage des colonnes du dataframe des events\n",
    "i = 0\n",
    "col_names = {}\n",
    "for col in range(events_split_df.columns.start, events_split_df.columns.stop):\n",
    "    col_names[col] = 'event_'+str(col)\n",
    "events_split_df.rename(columns=col_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impression du dataframe events\n",
    "events_split_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_split_df['event_0'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fractionne chaque event avec la liste des clés d'un dictionnaire event \n",
    "events_dict_split_df = pd.DataFrame()\n",
    "i = 0\n",
    "for col in events_split_df.columns :\n",
    "    events_dict_split_df[[\n",
    "        'source_'+str(col),\n",
    "        'message_'+str(col),\n",
    "        'timestamp_'+str(col),\n",
    "        'criticality_'+str(col),\n",
    "        'identification_'+str(col)\n",
    "        ]] = pd.DataFrame(events_split_df.iloc[:,i].tolist(), index= events_split_df.index)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_dict_split_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe des colonnes 'identification_'\n",
    "identification_data = {}\n",
    "for col in events_dict_split_df.columns :\n",
    "    if 'identification_' in col :\n",
    "        identification_data[col] = events_dict_split_df[col].values\n",
    "identification_df = pd.DataFrame(identification_data, index=events_dict_split_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusionner les valeurs des colonnes identifications dans une liste d'entiers\n",
    "identification_df['events_id'] = identification_df.iloc[:,:].apply(lambda x: convert_str_to_int_list(x), axis=1)\n",
    "identification_df = identification_df['events_id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'un dataframe des colonnes 'criticality_'\n",
    "criticality_data = {}\n",
    "for col in events_dict_split_df.columns :\n",
    "    if 'criticality_' in col :\n",
    "        criticality_data[col] = events_dict_split_df[col].values\n",
    "criticality_df = pd.DataFrame(criticality_data, index=events_dict_split_df.index)\n",
    "criticality_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on print les différentes valeurs 'criticality' présentent dans chaque colonne\n",
    "for col in criticality_df.columns:\n",
    "    print(col, criticality_df[col].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion des dataframes des colonnes fracionnées\n",
    "merge_df =  pd.merge(\n",
    "                pd.merge(\n",
    "                    pd.merge(\n",
    "                        metrics_df, \n",
    "                        connected_operators_df, left_index=True, right_index=True), \n",
    "                    modules_df, left_index=True, right_index=True), \n",
    "                events_dict_split_df, left_index=True, right_index=True)\n",
    "# suppression des colonnes fractionnées\n",
    "merge_df.drop(['connected_operators','modules','events'], axis=1, inplace=True)\n",
    "# conversion de la colonne \"created_at\" au format Date\n",
    "merge_df['created_at'] = pd.to_datetime(merge_df['created_at'])\n",
    "# indexation du dataset avec les valeurs \"created_at\"\n",
    "merge_df.index = merge_df['created_at']\n",
    "# suppression de la colonne \"created_at\"\n",
    "del merge_df['created_at']\n",
    "# sauvegarde du dataset en csv\n",
    "merge_df.to_csv('../data/metrics/metrics_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset ciblage events 'identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset temporaire\n",
    "temp_df = pd.merge(\n",
    "                pd.merge(\n",
    "                    pd.merge(\n",
    "                        metrics_df, \n",
    "                        connected_operators_df, left_index=True, right_index=True), \n",
    "                    modules_df, left_index=True, right_index=True), \n",
    "                identification_df, left_index=True, right_index=True)\n",
    "# suppression des colonnes fractionnées\n",
    "temp_df.drop(['connected_operators','modules'], axis=1, inplace=True)\n",
    "# conversion de la colonne \"created_at\" au format Date\n",
    "temp_df['created_at'] = pd.to_datetime(temp_df['created_at'])\n",
    "# sauvegarde du dataset en csv\n",
    "temp_df.to_csv('../data/metrics/temp_metrics_df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(dataframe) :\n",
    "    dataframe.plot(figsize=(15, 6))\n",
    "    plt.savefig('../data/metrics/img/metrics_df.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation d'une copie du dataframe avec normalisation\n",
    "# copy the data\n",
    "df_max_scaled = temp_df[['created_at','varnishLevelsTargetvolume','varnishLevelsTotalvolume','VarnishPrinter_3DVarnishCounter','iFoil_TotalPagesCounter']].copy(deep=True)\n",
    "df_max_scaled.index = df_max_scaled['created_at']\n",
    "del df_max_scaled['created_at']\n",
    "# apply normalization techniques\n",
    "for column in df_max_scaled.columns:\n",
    "    df_max_scaled[column] = df_max_scaled[column]  / df_max_scaled[column].abs().max()\n",
    "      \n",
    "# view normalized data\n",
    "display(df_max_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_timeseries(df_max_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2315c9af7dedaeb0b2bf51504304a927c605b523f04dad98936c50abe500b408"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
